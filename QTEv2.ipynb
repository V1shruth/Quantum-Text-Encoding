{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2 of the QTE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qte_py310/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qiskit\n",
    "qiskit.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 1: Install and Imports\n",
    "###############################################################################\n",
    "\n",
    "# If running in Google Colab or a fresh environment, uncomment to install needed packages:\n",
    "# !pip install datasets transformers qiskit qiskit-ibm-runtime nltk --quiet\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Hugging Face Datasets & Transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# NLTK for tokenization (optional, since HF can also tokenize)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Qiskit\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit\n",
    "#from qiskit.providers.fake_provider import FakeAthens --> dont really need this because we are using a real backend\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RZGate, CRZGate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 - Dataset Integration (CulturaX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 2: Phase 1 - Dataset Integration (CulturaX)\n",
    "###############################################################################\n",
    "# \n",
    "# This cell demonstrates how to load, split, and preprocess the CulturaX dataset.\n",
    "# We'll show an example in English, but you can replace \"en\" with other languages\n",
    "# or iterate over multiple subsets for multilingual analysis.\n",
    "###############################################################################\n",
    "\n",
    "def load_culturax_dataset(language=\"en\", use_auth_token=True, sample_size=None):\n",
    "    \"\"\"\n",
    "    Loads and returns the CulturaX dataset in the specified language.\n",
    "    Optionally downsamples the dataset to 'sample_size' examples for testing.\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading CulturaX dataset from Hugging Face...\")\n",
    "\n",
    "    # If 'uonlp/CulturaX' requires authentication, ensure you have a valid token set.\n",
    "    dataset = load_dataset(\"uonlp/CulturaX\", language, use_auth_token=use_auth_token)\n",
    "    \n",
    "    # Example: split into train/test\n",
    "    # Some versions of CulturaX might have pre-defined splits. Adjust as needed.\n",
    "    if \"train\" not in dataset:\n",
    "        dataset = dataset[\"data\"].train_test_split(test_size=0.2)\n",
    "    else:\n",
    "        dataset = dataset\n",
    "    \n",
    "    train_ds = dataset[\"train\"]\n",
    "    test_ds = dataset[\"test\"]\n",
    "\n",
    "    # Optional: further downsample (if sample_size is provided)\n",
    "    if sample_size is not None:\n",
    "        logging.info(f\"Downsampling dataset to {sample_size} samples for quick tests.\")\n",
    "        train_ds = train_ds.select(range(min(sample_size, len(train_ds))))\n",
    "        test_ds = test_ds.select(range(min(sample_size // 4, len(test_ds))))  # smaller test\n",
    "\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "class TransformerEmbedder:\n",
    "    \"\"\"\n",
    "    A simple wrapper that uses a pre-trained Transformer (e.g., DistilBERT) from Hugging Face\n",
    "    to produce embeddings for each text.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", device=\"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        # Optional: Use NLTK or rely purely on HF tokenizer. We'll do a minimal approach here.\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def get_embeddings(self, text: str):\n",
    "        \"\"\"\n",
    "        Returns a 2D array of shape (seq_len, hidden_dim) representing subword embeddings.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with np.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        last_hidden = outputs.last_hidden_state.squeeze(0)  # (seq_len, hidden_dim)\n",
    "        return last_hidden.cpu().numpy()\n",
    "\n",
    "def preprocess_dataset(dataset, model_name=\"distilbert-base-uncased\", device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Applies the TransformerEmbedder to each entry of the dataset's 'text' column.\n",
    "    Stores the resulting embeddings in a new column 'embeddings'.\n",
    "    \"\"\"\n",
    "    embedder = TransformerEmbedder(model_name=model_name, device=device)\n",
    "    \n",
    "    def _process_batch(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        all_embeddings = []\n",
    "        for txt in texts:\n",
    "            # Extract embeddings\n",
    "            emb = embedder.get_embeddings(txt)\n",
    "            all_embeddings.append(emb)\n",
    "        batch[\"embeddings\"] = all_embeddings\n",
    "        return batch\n",
    "    \n",
    "    # We map the process onto the entire dataset (vectorized approach).\n",
    "    dataset = dataset.map(_process_batch, batched=True, batch_size=8)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 - Model Enhancement & Training Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 3: Phase 2 - Model Enhancement & Training Pipeline\n",
    "# \n",
    "# This includes:\n",
    "#   1) Dimensionality reduction (PCA).\n",
    "#   2) Quantum circuits (amplitude encoding, positional encoding).\n",
    "#   3) Hierarchical grouping and quantum entangling.\n",
    "###############################################################################\n",
    "\n",
    "class DimensionalityReducer:\n",
    "    \"\"\"\n",
    "    PCA-based approach to reduce embeddings to a dimension = 2^n_qubits.\n",
    "    If the user wants target_power=3 => 2^3=8 dims, for example.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_power=3):\n",
    "        self.target_dim = 2 ** target_power\n",
    "        self.pca = PCA(n_components=self.target_dim)\n",
    "\n",
    "    def fit_transform(self, embeddings):\n",
    "        # embeddings can be shape: (seq_len, hidden_dim)\n",
    "        seq_len = embeddings.shape[0]\n",
    "        if seq_len < self.target_dim:\n",
    "            self.pca.n_components = max(1, seq_len)\n",
    "            logging.warning(f\"Reducing PCA dimension to {self.pca.n_components} due to limited seq_len.\")\n",
    "        reduced = self.pca.fit_transform(embeddings)\n",
    "        return reduced\n",
    "\n",
    "\n",
    "class QuantumEncoder:\n",
    "    \"\"\"\n",
    "    Encodes a single vector (dim = 2^n_qubits) into an amplitude-encoded state,\n",
    "    adds sinusoidal positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits, backend=None):\n",
    "        self.n_qubits = n_qubits\n",
    "        if backend is None:\n",
    "            # By default, use a mock backend. We'll override for real quantum hardware.\n",
    "            backend = FakeAthens()\n",
    "        self.backend = backend\n",
    "        self.sampler = Sampler()\n",
    "\n",
    "    def amplitude_encode(self, vector):\n",
    "        # Trim or pad to 2^n_qubits\n",
    "        dim = 2 ** self.n_qubits\n",
    "        vec = vector\n",
    "        if len(vec) < dim:\n",
    "            tmp = np.zeros(dim)\n",
    "            tmp[: len(vec)] = vec\n",
    "            vec = tmp\n",
    "        elif len(vec) > dim:\n",
    "            vec = vec[:dim]\n",
    "        # Normalize\n",
    "        norm = np.linalg.norm(vec)\n",
    "        if norm < 1e-8:\n",
    "            vec = np.ones(dim) / np.sqrt(dim)\n",
    "        else:\n",
    "            vec /= norm\n",
    "        return vec\n",
    "\n",
    "    def prepare_circuit(self, vector):\n",
    "        qc = QuantumCircuit(self.n_qubits)\n",
    "        amp_vec = self.amplitude_encode(vector)\n",
    "        qc.initialize(amp_vec, range(self.n_qubits))\n",
    "        return qc\n",
    "\n",
    "    def sinusoidal_position_encoding(self, qc, position):\n",
    "        \"\"\"\n",
    "        Applies RZ rotations with angles derived from sin/cos of 'position'.\n",
    "        \"\"\"\n",
    "        for q_idx in range(self.n_qubits):\n",
    "            angle = np.sin(position*0.1 + q_idx) + np.cos(position*0.05 + q_idx)\n",
    "            angle *= (np.pi / 4.0)\n",
    "            qc.rz(angle, q_idx)\n",
    "\n",
    "    def encode_with_position(self, vector, position):\n",
    "        qc = self.prepare_circuit(vector)\n",
    "        self.sinusoidal_position_encoding(qc, position)\n",
    "        return qc\n",
    "\n",
    "\n",
    "class ParametricEntangler:\n",
    "    \"\"\"\n",
    "    Demonstrates parametric CRZ gates between adjacent qubits. \n",
    "    You could train/tune these parameters in a hybrid quantum-classical loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.params = [Parameter(f\"theta_{i}_{i+1}\") for i in range(n_qubits-1)]\n",
    "\n",
    "    def apply(self, qc: QuantumCircuit, param_values=None):\n",
    "        if param_values is None:\n",
    "            param_values = [np.random.uniform(0, np.pi/2) for _ in self.params]\n",
    "        for i in range(self.n_qubits - 1):\n",
    "            crz_gate = CRZGate(self.params[i])\n",
    "            qc.append(crz_gate, [i, i+1])\n",
    "            qc = qc.assign_parameters({self.params[i]: param_values[i]}, inplace=False)\n",
    "        return qc\n",
    "\n",
    "\n",
    "class HierarchicalQuantumEncoder:\n",
    "    \"\"\"\n",
    "    Splits a sequence of vectors -> groups them -> amplitude encodes each group \n",
    "    -> entangles groups -> optionally merges them at a higher level.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder: QuantumEncoder, group_size=2, multi_level=True):\n",
    "        self.base_encoder = base_encoder\n",
    "        self.n_qubits = base_encoder.n_qubits\n",
    "        self.group_size = group_size\n",
    "        self.multi_level = multi_level\n",
    "        self.entangler = ParametricEntangler(n_qubits=self.n_qubits)\n",
    "        self.sampler = base_encoder.sampler\n",
    "\n",
    "    def _merge_vectors(self, vectors):\n",
    "        # Simple average\n",
    "        return np.mean(vectors, axis=0) if len(vectors) else None\n",
    "\n",
    "    def build_group_circuit(self, group_vectors, group_positions):\n",
    "        merged_vec = self._merge_vectors(group_vectors)\n",
    "        avg_pos = np.mean(group_positions)\n",
    "        return self.base_encoder.encode_with_position(merged_vec, avg_pos)\n",
    "\n",
    "    def build_level_circuit(self, vectors, positions):\n",
    "        \"\"\"\n",
    "        1) Group the vectors\n",
    "        2) Build subcircuits\n",
    "        3) Compose them into a single circuit\n",
    "        4) Parametric entangle consecutive groups\n",
    "        \"\"\"\n",
    "        group_circuits = []\n",
    "        for start_idx in range(0, len(vectors), self.group_size):\n",
    "            end_idx = start_idx + self.group_size\n",
    "            sub_vecs = vectors[start_idx:end_idx]\n",
    "            sub_pos = positions[start_idx:end_idx]\n",
    "            group_circuits.append(self.build_group_circuit(sub_vecs, sub_pos))\n",
    "\n",
    "        num_groups = len(group_circuits)\n",
    "        total_qubits = num_groups * self.n_qubits\n",
    "        qc_level = QuantumCircuit(total_qubits)\n",
    "\n",
    "        # Place each group circuit\n",
    "        for i, sub_qc in enumerate(group_circuits):\n",
    "            offset = i * self.n_qubits\n",
    "            qc_level.compose(sub_qc, qubits=range(offset, offset+self.n_qubits), inplace=True)\n",
    "\n",
    "        # Entangle consecutive groups\n",
    "        for g_idx in range(num_groups - 1):\n",
    "            ctrl_q = (g_idx+1)*self.n_qubits - 1\n",
    "            targ_q = (g_idx+1)*self.n_qubits\n",
    "            # Param values\n",
    "            angle_value = np.random.uniform(0, np.pi/2)\n",
    "            # We'll just use the first param for demonstration\n",
    "            param_gate = CRZGate(self.entangler.params[0])\n",
    "            qc_level.append(param_gate, [ctrl_q, targ_q])\n",
    "            qc_level = qc_level.assign_parameters({self.entangler.params[0]: angle_value}, inplace=False)\n",
    "\n",
    "        return qc_level, group_circuits\n",
    "\n",
    "    def encode_multilevel(self, vectors, positions):\n",
    "        # First level\n",
    "        level1_qc, subcircuits = self.build_level_circuit(vectors, positions)\n",
    "        if not self.multi_level or len(subcircuits) <= 1:\n",
    "            level1_qc.measure_all()\n",
    "            job = self.sampler.run([level1_qc])\n",
    "            res = job.result()\n",
    "            quasi = res.quasi_dists[0]\n",
    "            probs = np.array([quasi.get(i, 0.0) for i in range(2**level1_qc.num_qubits)])\n",
    "            probs = probs / np.sum(probs) if np.sum(probs) > 0 else probs\n",
    "            return probs\n",
    "\n",
    "        # Second level example\n",
    "        # In a real pipeline, you'd store the merged vectors at each group to re-encode them.\n",
    "        # We'll do a placeholder approach.\n",
    "        group_level_vectors = []\n",
    "        group_level_positions = []\n",
    "        for i, _ in enumerate(subcircuits):\n",
    "            # Dummy spike vector\n",
    "            v = np.zeros(2**self.n_qubits)\n",
    "            v[0] = 1.0\n",
    "            group_level_vectors.append(v)\n",
    "            group_level_positions.append(i)\n",
    "\n",
    "        level2_qc, _ = self.build_level_circuit(group_level_vectors, group_level_positions)\n",
    "        level2_qc.measure_all()\n",
    "        job = self.sampler.run([level2_qc])\n",
    "        res = job.result()\n",
    "        quasi = res.quasi_dists[0]\n",
    "        probs = np.array([quasi.get(i, 0.0) for i in range(2**level2_qc.num_qubits)])\n",
    "        probs = probs / np.sum(probs) if np.sum(probs) > 0 else probs\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 - Quantum Backend Configuration & Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 4: Phase 3 - Quantum Backend Configuration & Batch Processing\n",
    "#\n",
    "# 1) Integrate with IBM Quantum or local simulator.\n",
    "# 2) Provide functions to process a dataset in batches, building quantum circuits.\n",
    "# 3) Provide a skeleton for parallelizing circuit execution on HPC or real quantum hardware.\n",
    "###############################################################################\n",
    "\n",
    "def get_ibm_backend(min_qubits=5):\n",
    "    \"\"\"\n",
    "    Example function to get a real IBM quantum backend with at least 'min_qubits' qubits available.\n",
    "    Adjust QiskitRuntimeService credentials as needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        service = QiskitRuntimeService(channel=\"ibm_quantum\")\n",
    "        backends = service.backends(filters=lambda b: not b.configuration().simulator\n",
    "                                    and b.configuration().num_qubits >= min_qubits\n",
    "                                    and b.status().operational==True)\n",
    "        if backends:\n",
    "            backend = sorted(backends, key=lambda b: b.status().pending_jobs)[0]\n",
    "            logging.info(f\"Using backend {backend.name}\")\n",
    "            return backend\n",
    "        else:\n",
    "            logging.warning(\"No suitable real backend found. Falling back to FakeAthens.\")\n",
    "            return FakeAthens()\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to load real IBM backend: {e}\")\n",
    "        return FakeAthens()\n",
    "\n",
    "\n",
    "def process_batch_with_quantum_encoder(batch_embeddings, n_qubits=3, group_size=4, multi_level=True, backend=None):\n",
    "    \"\"\"\n",
    "    Takes a list of (seq_len, embed_dim) arrays (one per example in the batch),\n",
    "    and produces final quantum probability distributions for each example.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    base_encoder = QuantumEncoder(n_qubits=n_qubits, backend=backend)\n",
    "    hierarchical_encoder = HierarchicalQuantumEncoder(\n",
    "        base_encoder=base_encoder, group_size=group_size, multi_level=multi_level\n",
    "    )\n",
    "\n",
    "    for emb_2d in batch_embeddings:\n",
    "        if emb_2d is None or len(emb_2d) == 0:\n",
    "            results.append(None)\n",
    "            continue\n",
    "        \n",
    "        # 1) Flatten or reduce each example's embeddings dimension to 2^n_qubits\n",
    "        #    We'll do an additional PCA step for each example\n",
    "        #    (Alternatively, you could do a single PCA over the entire dataset.)\n",
    "        reducer = DimensionalityReducer(target_power=n_qubits)\n",
    "        try:\n",
    "            reduced = reducer.fit_transform(emb_2d)  # shape => (seq_len, 2^n_qubits)\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"PCA Error: {ve}\")\n",
    "            results.append(None)\n",
    "            continue\n",
    "\n",
    "        # 2) positions = simple [0..seq_len-1]\n",
    "        positions = list(range(len(reduced)))\n",
    "\n",
    "        # 3) encode with hierarchical quantum circuit\n",
    "        prob_dist = hierarchical_encoder.encode_multilevel(reduced, positions)\n",
    "        results.append(prob_dist)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4 - Putting It All Together (Demo Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fc1ddc56d5445bb416b0c0e66d16de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/10009 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6922d883b9b0424f88dc19f9dd97c39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/10000 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60164ce6e1714279b9d0eea6a82d76fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_14_4.parquet:  41%|####1     | 136M/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b889b1ab005342b4bee6267218eb0f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_14_5.parquet:   0%|          | 0.00/326M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd025ffdb21b4afeba0146fe59b688e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_14_6.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPleIAs/common_corpus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/load.py:2151\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2151\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2161\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2162\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/builder.py:978\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m    977\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 978\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:49\u001b[0m, in \u001b[0;36mParquet._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one data file must be specified, but got data_files=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m dl_manager\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mextract_on_the_fly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m splits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split_name, files \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/download/download_manager.py:326\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[1;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/download/download_manager.py:159\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    157\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[0;32m--> 159\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading data files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/utils/py_utils.py:494\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    492\u001b[0m     num_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(v, types) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m iterable):\n\u001b[0;32m--> 494\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m         map_nested(\n\u001b[1;32m    496\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    497\u001b[0m             data_struct\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m    498\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    499\u001b[0m             parallel_min_length\u001b[38;5;241m=\u001b[39mparallel_min_length,\n\u001b[1;32m    500\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    501\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    502\u001b[0m             types\u001b[38;5;241m=\u001b[39mtypes,\n\u001b[1;32m    503\u001b[0m         )\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_proc \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_proc \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m<\u001b[39m parallel_min_length:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batched:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/utils/py_utils.py:495\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    492\u001b[0m     num_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(v, types) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m iterable):\n\u001b[1;32m    494\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 495\u001b[0m         \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_min_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_min_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_proc \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_proc \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m<\u001b[39m parallel_min_length:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batched:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/utils/py_utils.py:511\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m--> 511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    512\u001b[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/utils/py_utils.py:512\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 512\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/utils/py_utils.py:380\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/utils/py_utils.py:380\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/datasets/download/download_manager.py:206\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[0;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     max_workers \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    203\u001b[0m         config\u001b[38;5;241m.\u001b[39mHF_DATASETS_MULTITHREADING_MAX_WORKERS \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m20\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    204\u001b[0m     )  \u001b[38;5;66;03m# enable multithreading if files are small\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_identity\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# contains the ranks of subprocesses\u001b[39;49;00m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHF_DATASETS_STACK_MULTIPROCESSING_DOWNLOAD_PROGRESS_BARS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_identity\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[1;32m    222\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qte_py310/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"PleIAs/common_corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Common Corpus dataset metadata...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d9cb0a7ff94618a4eff1846bdd58eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/10009 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159f11a55b8d464db54c2e282626e197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/10000 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3823f066764c229ad312794ad4b392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_1.parquet:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51abcc386814e6e9f80022955f39cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_10.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72271ddd5fd49d5928c650f45a7855a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_2.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a106acfa3094d098beff883314f4eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_3.parquet:   0%|          | 0.00/326M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ca4b7b516141e1b073e0ec7eddc888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_4.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406f2f97ba794964b6f679040a5b2314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_5.parquet:   0%|          | 0.00/322M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e596590e0f8147d598f22ef6f9905349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_6.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1e8e3dde2e4a8199a10d4202ffe987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_7.parquet:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71c965f9b5a4a1caebb94b99654c9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_8.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e179efc146e448b2a3c51c1ba2b65558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_100_9.parquet:   0%|          | 0.00/334M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a5f622d63043a799cdbaf258f2c90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_1.parquet:   0%|          | 0.00/326M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6802f915254f4c73b088758ccc597952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_10.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1118e210ea464bdc94f1cbf74e93e2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_2.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb61bacc88746528c4aae8fc9fe50f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_3.parquet:   0%|          | 0.00/326M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59dfd105ad14d39b86ec1110469c06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_4.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b063a4742f6a459983b1b9b063955be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_5.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0430da7608714445b4da60e09b7aa044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_6.parquet:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccd2441cfc54a988efa7e1a75bf0874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_7.parquet:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d678717c584b119c4b26a76cccd85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_8.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9e78ded72e4abfa8459f7060ce8d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_10_9.parquet:   0%|          | 0.00/336M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd756935c4b4871ab5893c455f07278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_1.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d564efb37ef541e694c4de3b0915874c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_10.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dd21d3a58745c28bc4448809cff986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_2.parquet:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ed51b2465945a6bff676f6bf45564f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_3.parquet:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1de27e274b47cd8b0208dfcde25742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_4.parquet:   0%|          | 0.00/327M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50734e7cc1654550a13f6c9e3f7af177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_5.parquet:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e379f71eb75a41939e210bbcb2c49534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_6.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f250aa3fb8a445b781c1cdad07ce7bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_7.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403805390d9042a7a86eb079fd894960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_8.parquet:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60660a4d6e824d0781df56afd92c9bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_11_9.parquet:   0%|          | 0.00/327M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3c818634c54ad3937db7d6d116abda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_1.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112b0b79be3443d186955e2d04e397c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_10.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c375f8d12cf1494995c971206a8a3823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_2.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196b96c8172548b1a1fc6e8f0dd2fb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_3.parquet:   0%|          | 0.00/333M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5202c021274588a9250305ccc5e593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_4.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4101818815b54a9c8bd1c9397f0ee609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_5.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb2d859023941c8bb32d7f454b49673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_6.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a600bc3828244852ba0f70cdc6968085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_7.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4c315b641e4b768e846f25d0f492f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_8.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8835c3eeb9f54be2b792234fbf6e0688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_12_9.parquet:   0%|          | 0.00/334M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890f27719f7842e5bbc88cbb0fdeb723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_1.parquet:   0%|          | 0.00/324M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba3fc3a82c448839f36fe934f630046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_10.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b2bf21d2744006a7c589c0992def10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_2.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571035a72bcc4b8dbe42cabd2a150d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_3.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf343555daf54b0a907b5c66fdf4a125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_4.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584e1cb237a44940bdc08c54133d9155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_5.parquet:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053ace1369fe47f9abf9fc1dd58f8ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_6.parquet:   0%|          | 0.00/334M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecca8fc90c84fa1a1ea050b20bbac00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_7.parquet:   0%|          | 0.00/334M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808317f8a412479bb667a158144d6317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_8.parquet:   0%|          | 0.00/333M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1a8a353b7f434d9e735fd4149762ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_13_9.parquet:   0%|          | 0.00/327M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f37c6e80c544439f3f25ec3c89544f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_14_1.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83712fdcb65845aea56a2c6ea01eb013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_14_10.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e704c959ce42008ad88978e61ee339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_14_2.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800a0446df5b4f4b8485565c32d589da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_14_3.parquet:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d25e9f751534addb7338fd955de274d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subset_14_4.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Updated Cell: Load Metadata and Small Subset for Testing\n",
    "#\n",
    "#   1) Load a small portion of Common Corpus dataset (metadata or specific rows)\n",
    "#   2) Preprocess (HF embeddings for each text)\n",
    "#   3) Dimensionality reduce & quantum-encode in batches\n",
    "#   4) Compare classical & quantum results, etc.\n",
    "###############################################################################\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_common_corpus_metadata(sample_size=100):\n",
    "    \"\"\"\n",
    "    Load metadata or a small subset of the Common Corpus dataset.\n",
    "    :param sample_size: Number of samples to load for demonstration/testing purposes.\n",
    "    :return: Small subset of the dataset for testing.\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading Common Corpus dataset metadata...\")\n",
    "\n",
    "    # Load a small subset of the dataset\n",
    "    dataset = load_dataset(\"PleIAs/common_corpus\", split=f\"train[:{sample_size}]\")\n",
    "\n",
    "    logging.info(f\"Loaded {len(dataset)} samples from the dataset.\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset, model_name=\"distilbert-base-uncased\", device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Preprocesses a dataset using Hugging Face Transformers.\n",
    "    Extracts embeddings for each text sample.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Preprocessing dataset with model: {model_name}...\")\n",
    "    embedder = TransformerEmbedder(model_name=model_name, device=device)\n",
    "    processed_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"]  # Assuming the dataset contains a \"text\" field\n",
    "        embeddings = embedder.get_token_embeddings(text)\n",
    "        processed_data.append({\"text\": text, \"embeddings\": embeddings})\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def classical_baseline(embeddings):\n",
    "    \"\"\"\n",
    "    Example classical baseline function. \n",
    "    Computes the average of embeddings as a simple baseline.\n",
    "    \"\"\"\n",
    "    return np.mean(embeddings, axis=(0, 1)) if len(embeddings) else None\n",
    "\n",
    "\n",
    "def main_pipeline_demo(sample_size=10, use_ibm=False):\n",
    "    \"\"\"\n",
    "    Demonstrates the entire pipeline with minimal data.\n",
    "    1) Load Common Corpus dataset metadata or small subset\n",
    "    2) Preprocess to get Hugging Face embeddings\n",
    "    3) Quantum encode in batches\n",
    "    4) Compare with classical baseline\n",
    "    \"\"\"\n",
    "    # Step 1: Load metadata or subset\n",
    "    dataset = load_common_corpus_metadata(sample_size=sample_size)\n",
    "    \n",
    "    # Step 2: Preprocess the dataset\n",
    "    logging.info(\"Preprocessing dataset with Transformers...\")\n",
    "    dataset = preprocess_dataset(dataset, model_name=\"distilbert-base-uncased\", device=\"cpu\")\n",
    "\n",
    "    # Step 3: Choose the quantum backend\n",
    "    backend = get_ibm_backend(min_qubits=5) if use_ibm else FakeAthens()\n",
    "\n",
    "    # Step 4: Process batches with the quantum encoder\n",
    "    embeddings = [sample[\"embeddings\"] for sample in dataset]\n",
    "    quantum_results = process_batch_with_quantum_encoder(\n",
    "        batch_embeddings=embeddings,\n",
    "        n_qubits=3,\n",
    "        group_size=4,\n",
    "        multi_level=True,\n",
    "        backend=backend\n",
    "    )\n",
    "\n",
    "    # Step 5: Compute classical baseline\n",
    "    classical_result = classical_baseline(embeddings)\n",
    "\n",
    "    # Step 6: Log and display results\n",
    "    logging.info(\"===== SAMPLE OUTPUT =====\")\n",
    "    logging.info(f\"Classical baseline representation shape: {classical_result.shape if classical_result is not None else None}\")\n",
    "    for i, qr in enumerate(quantum_results[:3]):\n",
    "        if qr is not None:\n",
    "            logging.info(f\"Sample {i} -> Quantum prob dist length: {len(qr)}; sum={np.sum(qr):.3f}\")\n",
    "        else:\n",
    "            logging.info(f\"Sample {i} -> No quantum result (likely empty)\")\n",
    "\n",
    "    return quantum_results, classical_result\n",
    "\n",
    "# --- Let's run the pipeline on a small demonstration ---\n",
    "if __name__ == \"__main__\":\n",
    "    quantum_dists, cls_baseline = main_pipeline_demo(sample_size=100, use_ibm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qte_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
