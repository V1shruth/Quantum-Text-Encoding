{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Quantum Text Encoding Pipeline (notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.13.1' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Hugging Face Transformers for pre-trained embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Qiskit imports\n",
    "from qiskit import QuantumCircuit\n",
    "#from qiskit.providers.fake_provider import FakeAthens,  USE WHEN YOU RUNOUT OF IBM JOB TIME\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RZGate, CRZGate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer-Based Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedder:\n",
    "    \"\"\"\n",
    "    Uses a pre-trained Transformer (e.g., BERT, DistilBERT) from Hugging Face\n",
    "    to produce embeddings for tokens or entire sequences.\n",
    "    \n",
    "    By default, this class will:\n",
    "      1. Tokenize the input text with the model's tokenizer.\n",
    "      2. Run the model to get hidden states.\n",
    "      3. Return token-level embeddings (e.g., the final layer's CLS or average).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        :param model_name: A valid Hugging Face model name.\n",
    "        :param device: 'cpu' or 'cuda' if GPU is available.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\"\n",
    "        Basic text tokenization using NLTK (optional) plus the model's own tokenizer.\n",
    "        Note that the transformer tokenizer itself can handle subwords and special tokens.\n",
    "        \"\"\"\n",
    "        # For demonstration, we combine a simple NLTK tokenize with HF tokenization.\n",
    "        # Usually you'd just rely on self.tokenizer directly.\n",
    "        tokens_nltk = word_tokenize(text.lower())\n",
    "        return tokens_nltk\n",
    "\n",
    "    def get_token_embeddings(self, text: str):\n",
    "        \"\"\"\n",
    "        Returns an array of shape (num_subword_tokens, hidden_size),\n",
    "        i.e., embeddings for each subword token in the text.\n",
    "        \n",
    "        You could also:\n",
    "         - Return only the [CLS] embedding.\n",
    "         - Average all subword embeddings per token or per sentence.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with np.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Let's take the last hidden state:\n",
    "            last_hidden_state = outputs.last_hidden_state  # (batch_size=1, seq_len, hidden_dim)\n",
    "        # Convert to numpy\n",
    "        embeddings = last_hidden_state.squeeze(0).detach().cpu().numpy()\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionalityReducer:\n",
    "    \"\"\"\n",
    "    PCA-based approach to reduce embeddings to a dimension = 2^n_qubits (when possible),\n",
    "    enabling direct amplitude encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_power=2):\n",
    "        \"\"\"\n",
    "        :param target_power: target dimension = 2^(target_power).\n",
    "                             e.g. target_power=2 => dimension=4\n",
    "        \"\"\"\n",
    "        self.target_dim = 2 ** target_power\n",
    "        self.pca = PCA(n_components=self.target_dim)\n",
    "\n",
    "    def fit_transform(self, embeddings):\n",
    "        \"\"\"\n",
    "        :param embeddings: np.array of shape (num_tokens, embed_dim)\n",
    "        :return: np.array of shape (num_tokens, target_dim)\n",
    "        If num_tokens < target_dim, PCA adjusts automatically.\n",
    "        \"\"\"\n",
    "        num_samples = embeddings.shape[0]\n",
    "        if num_samples < self.target_dim:\n",
    "            self.pca.n_components = max(1, num_samples)\n",
    "            logging.warning(f\"Reduced PCA dimension to {self.pca.n_components} due to sample size.\")\n",
    "        reduced = self.pca.fit_transform(embeddings)\n",
    "        return reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Encoder (Amplitude + Sinusoidal Positional Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumEncoder:\n",
    "    \"\"\"\n",
    "    Encodes a single vector (dimension 2^n_qubits) into an n_qubit amplitude-encoded state.\n",
    "    Adds sinusoidal-based phase shifts to capture positional information.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.backend = FakeAthens()  # a mock backend for demonstration\n",
    "        self.sampler = Sampler()\n",
    "\n",
    "    def amplitude_encode(self, vector):\n",
    "        \"\"\"\n",
    "        Pads/truncates 'vector' to length 2^n_qubits, then normalizes.\n",
    "        \"\"\"\n",
    "        target_len = 2 ** self.n_qubits\n",
    "        if len(vector) < target_len:\n",
    "            tmp = np.zeros(target_len)\n",
    "            tmp[: len(vector)] = vector\n",
    "            vector = tmp\n",
    "        elif len(vector) > target_len:\n",
    "            vector = vector[:target_len]\n",
    "\n",
    "        norm = np.linalg.norm(vector)\n",
    "        if norm < 1e-9:\n",
    "            # fallback uniform\n",
    "            vector = np.ones(target_len) / np.sqrt(target_len)\n",
    "        else:\n",
    "            vector /= norm\n",
    "        return vector\n",
    "\n",
    "    def prepare_state(self, vector):\n",
    "        \"\"\"\n",
    "        Builds a circuit that amplitude-encodes the vector using 'initialize()'.\n",
    "        For large n_qubits, consider more efficient state-prep methods.\n",
    "        \"\"\"\n",
    "        qc = QuantumCircuit(self.n_qubits)\n",
    "        encoded_vec = self.amplitude_encode(vector)\n",
    "        qc.initialize(encoded_vec, range(self.n_qubits))\n",
    "        return qc\n",
    "\n",
    "    def sinusoidal_position_encoding(self, qc, position):\n",
    "        \"\"\"\n",
    "        Applies a set of RZ gates whose angles are derived from sin/cos of 'position'\n",
    "        to mimic Transformer-like positional embedding in the phase.\n",
    "        \"\"\"\n",
    "        for q_idx in range(self.n_qubits):\n",
    "            angle = np.sin(position * 0.1 + q_idx) + np.cos(position * 0.05 + q_idx)\n",
    "            angle *= (np.pi / 4.0)\n",
    "            qc.rz(angle, q_idx)\n",
    "\n",
    "    def encode_with_position(self, vector, position):\n",
    "        \"\"\"\n",
    "        :return: A QuantumCircuit encoding 'vector' + position-based phase shifts.\n",
    "        \"\"\"\n",
    "        qc = self.prepare_state(vector)\n",
    "        self.sinusoidal_position_encoding(qc, position)\n",
    "        return qc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Entangling Block + Hierarchical Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametricEntangler:\n",
    "    \"\"\"\n",
    "    Demonstrates a parametric entangling pattern (CRZ) among qubits in a block.\n",
    "    The parameters can be learned or randomly assigned. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits):\n",
    "        self.n_qubits = n_qubits\n",
    "        # One parameter per pair of adjacent qubits\n",
    "        self.params = [Parameter(f\"theta_{i}_{i+1}\") for i in range(n_qubits - 1)]\n",
    "\n",
    "    def apply(self, qc: QuantumCircuit, param_values=None):\n",
    "        \"\"\"\n",
    "        Applies CRZ gates between adjacent qubits.\n",
    "        :param param_values: array or dict of parameter values. \n",
    "                             If None, use random values or a default.\n",
    "        \"\"\"\n",
    "        if param_values is None:\n",
    "            param_values = [np.random.uniform(0, np.pi / 2) for _ in self.params]\n",
    "\n",
    "        for i in range(self.n_qubits - 1):\n",
    "            angle = param_values[i]\n",
    "            crz_gate = CRZGate(self.params[i])\n",
    "            qc.append(crz_gate, [i, i+1])\n",
    "            # Bind numeric angle\n",
    "            qc = qc.assign_parameters({self.params[i]: angle}, inplace=False)\n",
    "\n",
    "        return qc\n",
    "\n",
    "\n",
    "class HierarchicalQuantumEncoder:\n",
    "    \"\"\"\n",
    "    Organizes embeddings into groups -> builds amplitude-encoded subcircuits \n",
    "    -> optionally merges groups at multiple levels -> parametric entangling between groups.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder: QuantumEncoder, group_size=2, multi_level=True):\n",
    "        self.base_encoder = base_encoder\n",
    "        self.n_qubits = base_encoder.n_qubits\n",
    "        self.group_size = group_size\n",
    "        self.multi_level = multi_level\n",
    "        self.entangler = ParametricEntangler(n_qubits=self.n_qubits)\n",
    "        self.sampler = base_encoder.sampler\n",
    "\n",
    "    def _merge_vectors(self, vectors):\n",
    "        \"\"\"\n",
    "        Merge the vectors in a single group. \n",
    "        E.g., a simple average or a more advanced approach.\n",
    "        \"\"\"\n",
    "        if len(vectors) == 0:\n",
    "            return None\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    def build_group_circuit(self, group_vectors, group_positions):\n",
    "        \"\"\"\n",
    "        Merges the group into a single vector, \n",
    "        amplitude-encodes it, adds positional encoding based on average position.\n",
    "        \"\"\"\n",
    "        merged_vec = self._merge_vectors(group_vectors)\n",
    "        avg_pos = np.mean(group_positions)\n",
    "        sub_qc = self.base_encoder.encode_with_position(merged_vec, avg_pos)\n",
    "        return sub_qc\n",
    "\n",
    "    def build_level_circuit(self, vectors, positions):\n",
    "        \"\"\"\n",
    "        1) Partition embeddings into groups.\n",
    "        2) For each group, build subcircuit.\n",
    "        3) Combine subcircuits into a single circuit.\n",
    "        4) Entangle the group-circuits in a chain using parametric CRZ gates.\n",
    "        \"\"\"\n",
    "        group_circuits = []\n",
    "        for start_idx in range(0, len(vectors), self.group_size):\n",
    "            end_idx = start_idx + self.group_size\n",
    "            sub_vecs = vectors[start_idx:end_idx]\n",
    "            sub_pos = positions[start_idx:end_idx]\n",
    "            qc_group = self.build_group_circuit(sub_vecs, sub_pos)\n",
    "            group_circuits.append(qc_group)\n",
    "\n",
    "        num_groups = len(group_circuits)\n",
    "        total_qubits = num_groups * self.n_qubits\n",
    "        level_qc = QuantumCircuit(total_qubits)\n",
    "\n",
    "        # Place each group subcircuit in a block\n",
    "        for i, sub_qc in enumerate(group_circuits):\n",
    "            offset = i * self.n_qubits\n",
    "            level_qc.compose(sub_qc, qubits=range(offset, offset+self.n_qubits), inplace=True)\n",
    "\n",
    "        # Entangle consecutive groups\n",
    "        for g_idx in range(num_groups - 1):\n",
    "            ctrl_q = (g_idx + 1)*self.n_qubits - 1\n",
    "            targ_q = (g_idx + 1)*self.n_qubits\n",
    "            # We'll pick random angles or a small array\n",
    "            ent_angles = [np.random.uniform(0, np.pi/2) for _ in range(self.n_qubits - 1)]\n",
    "            # parametric entangling block on the boundary [ctrl_q, targ_q]\n",
    "            # For simplicity, we only apply it to that boundary (like a single CRZ).\n",
    "            # Or you can do a small loop if you'd like more gates.\n",
    "            # We'll reuse the first param if it's simpler.\n",
    "            crz_param_value = ent_angles[0]\n",
    "            crz_gate = CRZGate(self.entangler.params[0])\n",
    "            level_qc.append(crz_gate, [ctrl_q, targ_q])\n",
    "            level_qc = level_qc.assign_parameters({self.entangler.params[0]: crz_param_value}, inplace=False)\n",
    "\n",
    "        return level_qc, group_circuits\n",
    "\n",
    "    def encode_multilevel(self, vectors, positions):\n",
    "        \"\"\"\n",
    "        Builds a multi-level circuit. For demonstration, we do at most two levels.\n",
    "        If multi_level is False or there's only one group, we finalize at one level.\n",
    "        \"\"\"\n",
    "        # ----------------------  First Level  ----------------------\n",
    "        level1_qc, subcircuits = self.build_level_circuit(vectors, positions)\n",
    "        if (not self.multi_level) or len(subcircuits) <= 1:\n",
    "            level1_qc.measure_all()\n",
    "            result = self.sampler.run(level1_qc)\n",
    "            quasi = result.result().quasi_dists[0]\n",
    "            probs = np.array([quasi.get(i, 0.0) for i in range(2**level1_qc.num_qubits)])\n",
    "            probs = probs / np.sum(probs) if np.sum(probs) > 0 else probs\n",
    "            return probs\n",
    "\n",
    "        # ----------------------  Second Level  ----------------------\n",
    "        # For demonstration, we'll re-construct vectors from each group.\n",
    "        # In a real system, you'd store the merged vectors from build_group_circuit.\n",
    "        # We'll create placeholders for level-2 grouping.\n",
    "        group_level_vectors = []\n",
    "        group_positions = []\n",
    "        for i, _ in enumerate(subcircuits):\n",
    "            # placeholder, e.g. a single \"spike\" vector\n",
    "            v = np.zeros(2**self.n_qubits)\n",
    "            v[0] = 1.0\n",
    "            group_level_vectors.append(v)\n",
    "            group_positions.append(i)\n",
    "\n",
    "        level2_qc, _ = self.build_level_circuit(group_level_vectors, group_positions)\n",
    "        level2_qc.measure_all()\n",
    "        result = self.sampler.run(level2_qc)\n",
    "        quasi = result.result().quasi_dists[0]\n",
    "        probs = np.array([quasi.get(i, 0.0) for i in range(2**level2_qc.num_qubits)])\n",
    "        probs = probs / np.sum(probs) if np.sum(probs) > 0 else probs\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Demo Function and Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Transformer model 'distilbert-base-uncased' for embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Encoding text: Quantum computing in modern machine learning research\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtxt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[43mdemo_transformer_based_encoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# => dimension=4 after PCA\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_qubits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# => amplitude-encode 4D vectors\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal distribution (size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(distribution)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdistribution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mdemo_transformer_based_encoding\u001b[0;34m(text, hf_model_name, pca_power, n_qubits, group_size, multi_level)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m1. Extract embeddings from a Hugging Face Transformer.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m2. Reduce dimensionality to 2^n_qubits via PCA.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m3. Encode them hierarchically in a quantum circuit.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m4. Return final probability distribution.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Transformer model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m embedder \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEmbedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m tokens \u001b[38;5;241m=\u001b[39m embedder\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[1;32m     19\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mTransformerEmbedder.__init__\u001b[0;34m(self, model_name, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m:param model_name: A valid Hugging Face model name.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m:param device: 'cpu' or 'cuda' if GPU is available.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n",
      "File \u001b[0;32m/opt/anaconda3/envs/QTE/lib/python3.13/site-packages/transformers/utils/import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/QTE/lib/python3.13/site-packages/transformers/utils/import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "def demo_transformer_based_encoding(\n",
    "    text: str,\n",
    "    hf_model_name=\"distilbert-base-uncased\",\n",
    "    pca_power=2,            # 2^2 = 4 dims after PCA\n",
    "    n_qubits=2,             # must match pca_power => amplitude encoding dimension=4\n",
    "    group_size=2,\n",
    "    multi_level=True\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Extract embeddings from a Hugging Face Transformer.\n",
    "    2. Reduce dimensionality to 2^n_qubits via PCA.\n",
    "    3. Encode them hierarchically in a quantum circuit.\n",
    "    4. Return final probability distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(f\"Loading Transformer model '{hf_model_name}' for embeddings...\")\n",
    "    embedder = TransformerEmbedder(model_name=hf_model_name)\n",
    "    tokens = embedder.tokenize(text)\n",
    "    logging.info(f\"Tokens: {tokens}\")\n",
    "\n",
    "    # Step 1: Get subword embeddings from the transformer\n",
    "    embeddings = embedder.get_token_embeddings(text)  # shape: (seq_len, hidden_dim)\n",
    "    logging.info(f\"Raw embedding shape: {embeddings.shape}\")\n",
    "\n",
    "    # Step 2: PCA to 2^pca_power\n",
    "    reducer = DimensionalityReducer(target_power=pca_power)\n",
    "    reduced_vectors = reducer.fit_transform(embeddings)\n",
    "    logging.info(f\"Reduced embedding shape: {reduced_vectors.shape}\")\n",
    "\n",
    "    # Step 3: Hierarchical Quantum Encoding\n",
    "    base_encoder = QuantumEncoder(n_qubits=n_qubits)\n",
    "    hierarchical_encoder = HierarchicalQuantumEncoder(\n",
    "        base_encoder=base_encoder,\n",
    "        group_size=group_size,\n",
    "        multi_level=multi_level\n",
    "    )\n",
    "\n",
    "    positions = list(range(len(reduced_vectors)))  # simple position index\n",
    "    final_probs = hierarchical_encoder.encode_multilevel(reduced_vectors, positions)\n",
    "    return final_probs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_texts = [\n",
    "        \"Quantum computing in modern machine learning research\",\n",
    "        \"Artificial intelligence uses deep neural networks\",\n",
    "        \"Data science and big data analytics revolution\"\n",
    "    ]\n",
    "\n",
    "    for txt in sample_texts:\n",
    "        print(\"=======================================================\")\n",
    "        print(f\"Encoding text: {txt}\")\n",
    "        distribution = demo_transformer_based_encoding(\n",
    "            text=txt,\n",
    "            hf_model_name=\"distilbert-base-uncased\",\n",
    "            pca_power=2,    # => dimension=4 after PCA\n",
    "            n_qubits=2,     # => amplitude-encode 4D vectors\n",
    "            group_size=2,\n",
    "            multi_level=True\n",
    "        )\n",
    "        print(f\"Final distribution (size={len(distribution)}):\\n{distribution}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QTE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
